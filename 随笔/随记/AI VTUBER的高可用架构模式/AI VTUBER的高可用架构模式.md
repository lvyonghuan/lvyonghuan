这是对这段时间思考的一个总结性质的记录。在正式讨论之前，我们先扔出一个最基本的AI Vtuber工作流程：获取信息->大模型生成回应->语音合成->前端展示。

首先我们要明白一点，在现有阶段上，AI Vtuber实际上就是对大模型做了一个复杂的应用封装，其功能基本上离不开大模型。从理论上讲，以大模型为基准点，向上是input接口，向下即output接口。input的渠道是多样的，而output的渠道也是多样的。

但是在实际的操作过程中，我们并不可能以大模型为完全的基准点。举个例子：唱歌这个功能，它就已经绕开大模型了。如果以大模型为基准点架构input和output，对唱歌来说就会显得很别扭。

而另一方面，大模型多种多样，每一个大模型都可能拥有自己的配置参数。有通用的，也有不通用的。为了给予选择上的灵活性，也不应该以大模型为架构的基准点。

我们应该转变一下思路。借用一下`软件定义网络`里的北向接口和南向接口的概念，我们把基准点往上（input）叫北向，基准点往下（output）为南向。那么实际上大模型这一层应该在南向上。为什么？因为对于大模型来说，它本身就是一个处理流程上的一环了，也就是output上的一个节点。

那么这个基准点的作用应当是什么？它应该选择消息，然后发送给配置文件选择的大模型，让它生成回应。

这实质上基本是`GoAtuber 2.0`的后端架构模式了。不过受限我的水平，我很多地方还是写的浆糊一团——比如前面提到的唱歌，我一开始实际上是没计划的。此外，`GO`本身抛开空间占用等等优势，拿来做这个活确实是有点受罪了——当然优势也不是完全没有，比如低开销处处使用blabla。

在`GoAtuber 2.0`的架构模式中，前文提到的基准点实际上是一个根堆结构（我当时数据结构都没学，问ChatGPT什么结构可以自动筛选出高优先级的消息，然后就有了这个优先队列），用来将高优先级的消息发送给指定的LLM。当然从这句话我们已经可以瞥见架构问题了——没错，我最开始只计划把消息发送给LLM，根本没想过唱歌的事情。后来听了牛肉玉音放送之后才突然醒悟，然后搓了一坨屎山，至今没合并到主分支。

不过依然不应该否认根堆这个结构的优点，因为它真能把最大优先级的消息先送出去。`GO`很唐的点就在这里出现了，这是门静态语言，管你唱歌也好普通的String消息也罢，都得给我用一个结构传出去。当然计划好了也不是不行，一旦一开始没想好很容易就改成屎了。（PS：我写的时候本身也属于风格发展期（PS：现在某种程度上还是），很多地方的写法甚至不统一，导致我自己看也觉得恶心了）

之后我半年多没碰AI Vtuber项目了，甚至有两个月没咋看牛肉了（勒夫罪大恶极，，，）。一者前端有活干忙了，其次其他项目写得也比我好，三来我本身也有活摆了。总之我没理由折腾这轮子了。（而且就算到现在，以一个外行的观点，我对LLM依然深恶痛绝。这东西好用的基本只能靠云，而大多数人又基本没硬件条件部署一个本地的——比如我。我现在的配置也就一台m40 24g再加一张e5，基本上做不了本地环境的开发）

然后去耍AI绘图了。8月的时候flux出来，当时只有comfyui支持，然后就硬着头皮体验了一把工作流。我本身是懒得去画工作流，不过这东西的灵活性是可见一斑的。

前几天刚好又有人问我AI vtuber项目的事情，我回忆了一下，从根堆结构想起，然后就想到了comfyui的工作流模式（顺带一提，blender的渲染器工作流也差不多长这样）。

基于这一点，我们就可以再出发了。我们不妨把每一步都拆分了，把每一个服务都做成一个`node`，从而实现高解耦。而用户自行连线组合，只需要线连得上（也就是接口能吻合），那就是能跑（跑不了就panic嘛）。

对于每个工作流图来说，没有输入的`node`本身就是input源，没有输出的就是output端，其它的就是中间处理流程。简单画个示意图。
[工作流示意图](.\随笔\随记\AI VTUBER的高可用架构模式\工作流示意图.png)

就比如说过滤器，依靠`node`模式，我们可以自由选择每一个过滤流程具体使用什么过滤器，是传统的字典还是依靠大模型，乃至依靠专门的审核平台。把实现的复杂度都封装在`node`里，整体的组织就轻松许多了。我们本身也可以再做一些标记器、筛选器等节点，通过条件判断来给消息的有限度打标，最后实现流程整体的有机灵活运转。只需要在GUI层面上调整工作流就行了。

不过comfyui的那套我没找到一个开源的实现——从comfyui本身中剥离出来似乎也不是不行。不过谷歌本身基于JS做了一套叫`Blockly`的东西，看起来非常类似小学学的`Scratch`（没错，那只黄猫）。这样一来，似乎用JS作为一个大的语言方向也没啥问题了。用python也不是不行。

用Go和Rust就有点受罪了——当然也不是不行。

就这样吧。